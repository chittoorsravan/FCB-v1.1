name: FCB AttestationHub Server CI/CD

on:
  push:
    branches: [ "main" ]

env:
  AWS_REGION: us-east-1
  ECR_REPOSITORY: fcb-attestationhub-server
  EKS_CLUSTER_NAME: fcb-eks-cluster
  IMAGE_TAG: ${{ github.sha }}
  K8S_NAMESPACE: attestationhub
  APP_DEPLOYMENT: attestationhubserver

permissions:
  contents: read

jobs:
  build-test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-java@v4
        with:
          distribution: temurin
          java-version: "8"
          cache: maven

      # Create local folders and perms (as you requested)
      - name: Prepare /data directory
        run: |
          sudo mkdir -p /data
          sudo chmod 777 /data
          mkdir -p target/h2data

      - name: Maven Build (skip tests for speed)
        run: mvn -B -ntp clean package -DskipTests

      - name: Maven Tests
        run: mvn -B -ntp test

  security-check:
    runs-on: ubuntu-latest
    needs: build-test
    steps:
      - uses: actions/checkout@v4

      - name: Install Trivy
        uses: aquasecurity/setup-trivy@v0.2.2

      - name: Trivy FS Scan
        run: trivy fs --exit-code 0 --format json -o trivy-fs.json .

      - name: Install Gitleaks
        uses: zricethezav/gitleaks-action@v2
        with:
          args: detect --source . -v -f json -r gitleaks-report.json
        continue-on-error: true

      - uses: actions/upload-artifact@v4
        with:
          name: security-reports
          path: |
            trivy-fs.json
            gitleaks-report.json

  docker-build-push:
    runs-on: ubuntu-latest
    needs: [ build-test, security-check ]
    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id:     ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region:            ${{ env.AWS_REGION }}

      - name: Login to ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Ensure ECR repo exists
        run: |
          aws ecr describe-repositories --repository-names "$ECR_REPOSITORY" >/dev/null 2>&1 || \
          aws ecr create-repository --repository-name "$ECR_REPOSITORY"

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build & Push (SHA + latest)
        env:
          REGISTRY: ${{ steps.login-ecr.outputs.registry }}
        run: |
          IMAGE_BASE="${REGISTRY}/${ECR_REPOSITORY}"
          docker buildx build --platform linux/amd64 \
            -t "${IMAGE_BASE}:${IMAGE_TAG}" \
            -t "${IMAGE_BASE}:latest" \
            --push .

  deploy-app:
    runs-on: ubuntu-latest
    needs: docker-build-push
    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id:     ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region:            ${{ env.AWS_REGION }}

      - name: Setup kubectl
        uses: azure/setup-kubectl@v4

      - name: Update kubeconfig
        run: aws eks update-kubeconfig --name "$EKS_CLUSTER_NAME" --region "$AWS_REGION"

      - name: Bump IP capacity on node (AWS CNI prefix delegation)
        run: |
          set -euxo pipefail
          kubectl -n kube-system set env daemonset/aws-node ENABLE_PREFIX_DELEGATION=true WARM_PREFIX_TARGET=1
          kubectl -n kube-system rollout status ds/aws-node --timeout=5m

      - name: Login to ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Apply namespaces
        run: kubectl apply -f k8s/namespaces.yaml

      # Ensure PriorityClass exists, but never mutate it to avoid "value: Forbidden"
      - name: Ensure PriorityClass exists (no updates)
        run: |
          set -euo pipefail
          if ! kubectl get priorityclass app-high >/dev/null 2>&1; then
            cat <<'EOF' | kubectl apply -f -
            apiVersion: scheduling.k8s.io/v1
            kind: PriorityClass
            metadata:
              name: app-high
            value: 100000000
            globalDefault: false
            description: "High priority for attestationhub app"
            EOF
          else
            echo "PriorityClass app-high already exists; leaving as-is."
          fi

      # Free up the node before applying the new Pod to avoid "Too many pods"
      - name: Pre-clear app pods (scale down & delete leftovers)
        run: |
          set -euxo pipefail
          kubectl -n ${K8S_NAMESPACE} scale deploy/${APP_DEPLOYMENT} --replicas=0 || true
          kubectl -n ${K8S_NAMESPACE} delete pod -l app=${APP_DEPLOYMENT} --ignore-not-found
          # Nuke obviously bad pods (Evicted, CrashLoop, etc.) if any remain
          kubectl -n ${K8S_NAMESPACE} get pods --no-headers | \
            awk '/Evicted|Error|ImagePullBackOff|CrashLoopBackOff/ {print $1}' | \
            xargs -r kubectl -n ${K8S_NAMESPACE} delete pod

      - name: Patch image & deploy app (Deployment + Service)
        env:
          REGISTRY: ${{ steps.login-ecr.outputs.registry }}
        run: |
          set -euxo pipefail
          IMAGE_URI="${REGISTRY}/${ECR_REPOSITORY}:${IMAGE_TAG}"
          echo "Deploying image: $IMAGE_URI"
          # Patch image in your checked-in manifest
          sed -i -E "s|(^[[:space:]]*image:[[:space:]]*).*$|\1${IMAGE_URI}|g" k8s/app/deployment.yaml

          # Ensure deployment references the PriorityClass (merge patch)
          kubectl -n "${K8S_NAMESPACE}" patch deploy/${APP_DEPLOYMENT} --type merge -p '{
            "spec": { "template": { "spec": { "priorityClassName": "app-high" } } }
          }' || true

          kubectl apply -f k8s/app/deployment.yaml
          kubectl apply -f k8s/app/service.yaml

          # Scale back up
          kubectl -n ${K8S_NAMESPACE} scale deploy/${APP_DEPLOYMENT} --replicas=1
          kubectl rollout status deployment/${APP_DEPLOYMENT} -n ${K8S_NAMESPACE} --timeout=8m

      - name: On failure dump diagnostics
        if: ${{ failure() }}
        run: |
          set -euxo pipefail
          echo '=== Deployment describe ==='
          kubectl describe deploy/${APP_DEPLOYMENT} -n ${K8S_NAMESPACE}
          echo '=== Pods (wide) ==='
          kubectl get pods -n ${K8S_NAMESPACE} -o wide
          echo '=== Events (last 200) ==='
          kubectl get events -n ${K8S_NAMESPACE} --sort-by=.lastTimestamp | tail -n 200
          echo '=== Node capacity & allocatable ==='
          N=$(kubectl get nodes -o 'jsonpath={.items[0].metadata.name}')
          kubectl describe node "$N" | sed -n 1,200p | grep -E 'pods:|Capacity|Allocatable' -n
          echo '=== Pods on the node ==='
          kubectl get pods -A -o wide --field-selector spec.nodeName=$N
          echo '=== aws-cni logs (recent) ==='
          kubectl -n kube-system logs ds/aws-node --tail=200 || true

      - name: Show App NLB DNS
        run: |
          echo "App Service (NLB) DNS:"
          kubectl get svc attestationhubserver-svc -n ${K8S_NAMESPACE} \
            -o jsonpath='{.status.loadBalancer.ingress[0].hostname}'; echo
          echo "Open: http://<above-host>/attestationhubserver/swagger-ui/index.html"

  deploy-observability:
    runs-on: ubuntu-latest
    needs: deploy-app
    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id:     ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region:            ${{ env.AWS_REGION }}

      - name: Setup kubectl
        uses: azure/setup-kubectl@v4

      - name: Setup Helm
        uses: azure/setup-helm@v4

      - name: Update kubeconfig
        run: aws eks update-kubeconfig --name "$EKS_CLUSTER_NAME" --region "$AWS_REGION"

      # Write a minimal, no-PVC values file inline
      - name: Write kps-values.yaml (no PVCs)
        run: |
          mkdir -p k8s/monitoring
          cat > k8s/monitoring/kps-values.yaml <<'YAML'
          grafana:
            adminPassword: "ChangeMe123!"
            service:
              type: LoadBalancer
            persistence:
              enabled: false
          prometheus:
            prometheusSpec:
              retention: 24h
              storageSpec: null
            service:
              type: LoadBalancer
          YAML

      # Temporarily scale monitoring down to free pods/IPs (ignore node-exporter)
      - name: Temporarily scale monitoring down
        run: |
          set -euxo pipefail
          sudo apt-get update && sudo apt-get install -y jq
          SNAP=/tmp/monitoring-scale-snapshot.json
          kubectl -n monitoring get deploy,sts -o json > "$SNAP" || true
          kubectl -n monitoring scale deploy --all --replicas=0 || true
          kubectl -n monitoring scale sts --all --replicas=0 || true
          # Wait for everything but node-exporter to disappear
          if kubectl -n monitoring get pods --no-headers >/dev/null 2>&1; then
            kubectl -n monitoring wait --for=delete pod \
              -l 'app notin (prometheus-node-exporter)' \
              --timeout=5m || true
          fi

      # Heal stuck Helm release states, then install/upgrade with retry
      - name: Heal stuck Helm release (kps) if needed
        run: |
          set -euo pipefail
          kubectl create namespace monitoring --dry-run=client -o yaml | kubectl apply -f -

          if helm -n monitoring status kps >/dev/null 2>&1; then
            STATUS=$(helm -n monitoring status kps -o json | jq -r '.info.status')
            echo "kps current status: ${STATUS}"
            if [[ "${STATUS}" == pending* ]]; then
              echo "Release pending; attempting rollbackâ€¦"
              LAST=$(helm -n monitoring history kps -o json | jq -r '[.[]|select(.status=="deployed")]|last|.revision // empty')
              if [[ -n "${LAST}" ]]; then
                helm -n monitoring rollback kps "${LAST}" --wait --timeout 10m || true
              else
                echo "No deployed revision; uninstalling and cleaning Helm recordsâ€¦"
                helm -n monitoring uninstall kps || true
                kubectl -n monitoring delete secret -l owner=helm,name=kps || true
                kubectl -n monitoring delete configmap -l owner=helm,name=kps || true
              fi
            fi
          fi

          # Clean stuck hook artifacts (best effort)
          kubectl -n monitoring delete job -l app=kube-prometheus-stack --ignore-not-found || true
          kubectl -n monitoring delete pod -l app=kube-prometheus-stack,owner=helm \
            --field-selector=status.phase!=Running --ignore-not-found || true

          # Wait until release is not pending
          for i in {1..30}; do
            S=$(helm -n monitoring ls -o json | jq -r '.[]|select(.name=="kps")|.status // "none"')
            [[ -z "$S" || "$S" == "deployed" || "$S" == "uninstalled" ]] && break
            if [[ "$S" == pending* ]]; then
              echo "kps still $S; waitingâ€¦"; sleep 10
            else
              break
            fi
          done

      - name: Install/Upgrade kube-prometheus-stack (no PVCs, with retry)
        env:
          GRAFANA_ADMIN_PW: ${{ secrets.GRAFANA_ADMIN_PW }}
        run: |
          set -euxo pipefail
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
          helm repo update

          ATTEMPTS=3
          for n in $(seq 1 $ATTEMPTS); do
            if helm upgrade --install kps prometheus-community/kube-prometheus-stack \
              --namespace monitoring \
              -f k8s/monitoring/kps-values.yaml \
              --set grafana.adminPassword="${GRAFANA_ADMIN_PW:-ChangeMe123!}" \
              --wait --timeout 15m --atomic; then
              exit 0
            fi
            echo "Attempt $n failed; cleaning pending state and retryingâ€¦"
            helm -n monitoring uninstall kps || true
            kubectl -n monitoring delete secret -l owner=helm,name=kps || true
            kubectl -n monitoring delete configmap -l owner=helm,name=kps || true
            sleep 10
          done
          echo "Helm install failed after $ATTEMPTS attempts"; exit 1

      - name: Scale monitoring back up (using saved snapshot if present)
        run: |
          set -euxo pipefail
          SNAP=/tmp/monitoring-scale-snapshot.json
          if [[ -f "$SNAP" ]]; then
            while IFS=$'\t' read -r name replicas; do
              [[ -z "$name" ]] && continue
              kubectl -n monitoring scale deploy "$name" --replicas="${replicas}"
            done < <(jq -r '.items[] | select(.kind=="Deployment") | [.metadata.name, (.spec.replicas//1)] | @tsv' "$SNAP")

            while IFS=$'\t' read -r name replicas; do
              [[ -z "$name" ]] && continue
              kubectl -n monitoring scale sts "$name" --replicas="${replicas}"
            done < <(jq -r '.items[] | select(.kind=="StatefulSet") | [.metadata.name, (.spec.replicas//1)] | @tsv' "$SNAP")
          else
            # Sensible defaults if no snapshot existed
            kubectl -n monitoring scale deploy kps-grafana --replicas=1 || true
            kubectl -n monitoring scale deploy kps-kube-prometheus-stack-operator --replicas=1 || true
            kubectl -n monitoring scale deploy kps-kube-state-metrics --replicas=1 || true
            kubectl -n monitoring scale sts alertmanager-kps-kube-prometheus-stack-alertmanager --replicas=1 || true
            kubectl -n monitoring scale sts prometheus-kps-kube-prometheus-stack-prometheus --replicas=1 || true
          fi

      - name: Show Observability LBs
        run: |
          echo "Grafana:"
          kubectl get svc kps-grafana -n monitoring -o jsonpath='{.status.loadBalancer.ingress[0].hostname}'; echo
          echo "Prometheus:"
          kubectl get svc kps-kube-prometheus-prometheus -n monitoring -o jsonpath='{.status.loadBalancer.ingress[0].hostname}'; echo
