name: FCB AttestationHub Server CI/CD

on:
  push:
    branches: [ "main" ]

env:
  AWS_REGION: us-east-1
  ECR_REPOSITORY: fcb-attestationhub-server
  EKS_CLUSTER_NAME: fcb-eks-cluster
  IMAGE_TAG: ${{ github.sha }}
  K8S_NAMESPACE: attestationhub
  APP_DEPLOYMENT: attestationhubserver
  ENABLE_PREFIX_DELEGATION: "true"
  TEMP_SCALE_DOWN_MONITORING: "true"

permissions:
  contents: read

jobs:
  build-test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-java@v4
        with:
          distribution: temurin
          java-version: "8"
          cache: maven
      - name: Prepare /data directory
        run: |
          sudo mkdir -p /data
          sudo chmod 777 /data
          mkdir -p target/h2data
      - name: Maven Build (skip tests for speed)
        run: mvn -B -ntp clean package -DskipTests
      - name: Maven Tests
        run: mvn -B -ntp test

  security-check:
    runs-on: ubuntu-latest
    needs: build-test
    steps:
      - uses: actions/checkout@v4
      - name: Install Trivy
        uses: aquasecurity/setup-trivy@v0.2.2
      - name: Trivy FS Scan
        run: trivy fs --exit-code 0 --format json -o trivy-fs.json .
      - name: Install Gitleaks
        uses: zricethezav/gitleaks-action@v2
        with:
          args: detect --source . -v -f json -r gitleaks-report.json
        continue-on-error: true
      - uses: actions/upload-artifact@v4
        with:
          name: security-reports
          path: |
            trivy-fs.json
            gitleaks-report.json

  docker-build-push:
    runs-on: ubuntu-latest
    needs: [ build-test, security-check ]
    steps:
      - uses: actions/checkout@v4
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id:     ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region:            ${{ env.AWS_REGION }}
      - name: Login to ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2
      - name: Ensure ECR repo exists
        run: |
          aws ecr describe-repositories --repository-names "$ECR_REPOSITORY" >/dev/null 2>&1 || \
          aws ecr create-repository --repository-name "$ECR_REPOSITORY"
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      - name: Build & Push (SHA + latest)
        env:
          REGISTRY: ${{ steps.login-ecr.outputs.registry }}
        run: |
          IMAGE_BASE="${REGISTRY}/${ECR_REPOSITORY}"
          docker buildx build --platform linux/amd64 \
            -t "${IMAGE_BASE}:${IMAGE_TAG}" \
            -t "${IMAGE_BASE}:latest" \
            --push .

  deploy:
    runs-on: ubuntu-latest
    needs: docker-build-push
    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id:     ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region:            ${{ env.AWS_REGION }}

      - name: Setup kubectl
        uses: azure/setup-kubectl@v4
      - name: Setup Helm
        uses: azure/setup-helm@v4
      - name: Update kubeconfig
        run: aws eks update-kubeconfig --name "$EKS_CLUSTER_NAME" --region "$AWS_REGION"

      - name: Install jq (for snapshot/restore)
        run: |
          sudo apt-get update -y
          sudo apt-get install -y jq

      - name: Login to ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Apply namespaces
        run: kubectl apply -f k8s/namespaces.yaml

      - name: Enable AWS VPC CNI Prefix Delegation (optional)
        if: env.ENABLE_PREFIX_DELEGATION == 'true'
        run: |
          kubectl -n kube-system set env daemonset/aws-node ENABLE_PREFIX_DELEGATION=true WARM_PREFIX_TARGET=1
          kubectl -n kube-system rollout status ds/aws-node --timeout=5m

      - name: Ensure PriorityClass app-high (create-only; no heredoc)
        run: |
          if ! kubectl get priorityclass app-high >/dev/null 2>&1; then
            F=$(mktemp)
            printf '%s\n' \
'apiVersion: scheduling.k8s.io/v1' \
'kind: PriorityClass' \
'metadata:' \
'  name: app-high' \
'value: 100000' \
'globalDefault: false' \
'description: "High priority for AttestationHub app"' \
> "$F"
            kubectl apply -f "$F"
          fi

      - name: Ensure monitoring values (no PVCs) file exists (no heredoc)
        run: |
          mkdir -p k8s/monitoring
          F=k8s/monitoring/kps-values.yaml
          : > "$F"
          printf '%s\n' \
'grafana:' \
'  adminPassword: "ChangeMe123!"' \
'  service:' \
'    type: LoadBalancer' \
'  persistence:' \
'    enabled: false' \
'' \
'prometheus:' \
'  prometheusSpec:' \
'    retention: 24h' \
'    storageSpec: null' \
'  service:' \
'    type: LoadBalancer' \
>> "$F"
          echo "Wrote $F"

      - name: Snapshot & scale down monitoring (free pod/IP capacity)
        if: env.TEMP_SCALE_DOWN_MONITORING == 'true'
        run: |
          SNAP=/tmp/monitoring-scale-snapshot.json
          kubectl -n monitoring get deploy,sts -o json > "$SNAP" || echo '{"items":[]}' > "$SNAP"
          kubectl -n monitoring scale deploy --all --replicas=0 || true
          kubectl -n monitoring scale sts --all --replicas=0 || true
          # wait for non-daemonset pods to go away
          kubectl -n monitoring get pods --no-headers || true
          kubectl -n monitoring wait --for=delete pod -l 'app notin (prometheus-node-exporter)' --timeout=5m || true

      - name: Heal Helm release if stuck (kps)
        run: |
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
          helm repo update
          if helm status kps -n monitoring >/dev/null 2>&1; then
            s=$(helm status kps -n monitoring -o json | jq -r '.info.status' || echo unknown)
            echo "kps status: $s"
            if echo "$s" | grep -Eiq 'pending|failed'; then
              kubectl -n monitoring get secret -o name | \
                grep -E '^secret/sh\.helm\.release\.v1\.kps\.v' | \
                xargs -r kubectl -n monitoring delete
            fi
          else
            echo "kps not installed yet; ok"
          fi

      - name: Install/Upgrade kube-prometheus-stack (no PVCs)
        run: |
          kubectl create namespace monitoring --dry-run=client -o yaml | kubectl apply -f -
          helm upgrade --install kps prometheus-community/kube-prometheus-stack \
            --namespace monitoring \
            -f k8s/monitoring/kps-values.yaml \
            --set grafana.adminPassword="ChangeMe123!" \
            --wait --timeout 15m --atomic

      - name: Scale app to 0 & clear leftovers (avoid “Too many pods”)
        run: |
          kubectl -n ${K8S_NAMESPACE} scale deploy/${APP_DEPLOYMENT} --replicas=0 || true
          kubectl -n ${K8S_NAMESPACE} delete pod -l app=${APP_DEPLOYMENT} --ignore-not-found || true
          # delete any non-running leftovers
          for p in $(kubectl get pods -n ${K8S_NAMESPACE} \
            --field-selector=status.phase!=Running \
            -o jsonpath='{.items[*].metadata.name}'); do
            kubectl -n ${K8S_NAMESPACE} delete pod "$p" || true
          done

      - name: Patch image & apply manifests (idempotent)
        env:
          REGISTRY: ${{ steps.login-ecr.outputs.registry }}
        run: |
          IMAGE_URI="${REGISTRY}/${ECR_REPOSITORY}:${IMAGE_TAG}"
          echo "Deploying image: ${IMAGE_URI}"
          sed -i -E "s|(^[[:space:]]*image:[[:space:]]*).*$|\1${IMAGE_URI}|g" k8s/app/deployment.yaml
          # ensure priorityClassName in deployment if not present (simple insert)
          if ! grep -q 'priorityClassName:' k8s/app/deployment.yaml; then
            awk '
              {print}
              /template:[[:space:]]*$/ {intmpl=1}
              intmpl && /spec:[[:space:]]*$/ {print "      priorityClassName: app-high"; intmpl=0}
            ' k8s/app/deployment.yaml > k8s/app/deployment.yaml.new && mv k8s/app/deployment.yaml.new k8s/app/deployment.yaml || true
          fi
          kubectl apply -f k8s/app/deployment.yaml
          kubectl apply -f k8s/app/service.yaml

      - name: Scale app up and wait for rollout
        run: |
          kubectl -n ${K8S_NAMESPACE} scale deploy/${APP_DEPLOYMENT} --replicas=1
          kubectl rollout status deployment/${APP_DEPLOYMENT} -n ${K8S_NAMESPACE} --timeout=8m

      - name: Restore monitoring replicas
        if: env.TEMP_SCALE_DOWN_MONITORING == 'true'
        run: |
          SNAP=/tmp/monitoring-scale-snapshot.json
          if [[ -f "$SNAP" ]]; then
            jq -r '.items[] | select(.kind=="Deployment") | [.metadata.name, (.spec.replicas//1)] | @tsv' "$SNAP" | \
            while IFS=$'\t' read -r n r; do
              [[ -n "$n" ]] && kubectl -n monitoring scale deploy "$n" --replicas="${r:-1}" || true
            done
            jq -r '.items[] | select(.kind=="StatefulSet") | [.metadata.name, (.spec.replicas//1)] | @tsv' "$SNAP" | \
            while IFS=$'\t' read -r n r; do
              [[ -n "$n" ]] && kubectl -n monitoring scale sts "$n" --replicas="${r:-1}" || true
            done
          else
            kubectl -n monitoring scale deploy --all --replicas=1 || true
            kubectl -n monitoring scale sts --all --replicas=1 || true
          fi

      - name: Show Endpoints
        run: |
          echo "App (NLB) DNS:"
          kubectl get svc attestationhubserver-svc -n ${K8S_NAMESPACE} -o jsonpath='{.status.loadBalancer.ingress[0].hostname}'; echo
          echo "Grafana:"
          kubectl get svc kps-grafana -n monitoring -o jsonpath='{.status.loadBalancer.ingress[0].hostname}'; echo
          echo "Prometheus:"
          kubectl get svc kps-kube-prometheus-prometheus -n monitoring -o jsonpath='{.status.loadBalancer.ingress[0].hostname}'; echo

      - name: On failure: dump diagnostics
        if: ${{ failure() }}
        run: |
          echo "=== Pods (all) ==="; kubectl get pods -A -o wide || true
          echo "=== App events ==="; kubectl get events -n ${K8S_NAMESPACE} --sort-by=.lastTimestamp | tail -n 200 || true
          echo "=== App deploy ==="; kubectl describe deploy/${APP_DEPLOYMENT} -n ${K8S_NAMESPACE} || true
          P=$(kubectl get pods -n ${K8S_NAMESPACE} -l app=${APP_DEPLOYMENT} -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || true)
          [ -n "$P" ] && kubectl describe pod "$P" -n ${K8S_NAMESPACE} || true
          [ -n "$P" ] && kubectl logs "$P" -n ${K8S_NAMESPACE} --tail=200 || true
          echo "=== Node/pods ==="
          N=$(kubectl get nodes -o jsonpath='{.items[0].metadata.name}' || true)
          [ -n "$N" ] && kubectl describe node "$N" | sed -n '1,200p' | grep -nE 'pods:|Capacity|Allocatable' || true
          [ -n "$N" ] && kubectl get pods -A -o wide --field-selector spec.nodeName="$N" || true
          echo "=== aws-cni logs ==="; kubectl -n kube-system logs ds/aws-node --tail=200 || true
